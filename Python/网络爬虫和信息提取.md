# 001-网络爬虫的规则

日期：20190825    时长：150MS    累计：150MS         

## 内容大纲

Requests库入门、爬虫的“盗亦有道”、Requests库实战练习

## 内容详解

- Requests库入门

  - requests.request() 构造一个请求，支撑以下各方法的基础方法

  - requests.get() 获取HTML网页的主要方法

    - ```
      r= requests.get(url)
      #5个response对象的属性
      r.status_code  #HTTP请求的返回状态，200链接成功，404表示失败
      r.text  #url对应的网页内容
      r.encoding  #从HTTP header中猜测的响应内容编码方式，如果header中不存在charset，则认为编码为ISO-8859-1
      r.apparent_encoding  #从内容中分析出的响应内容编码方式(备选)，原则讲比r.encoding更准确
      r.content # HTTP响应内容的二进制形式
      ```

    - requests.get(url,params=None,**kwargs)，url页面链接，params是url中的额外参数，可选，kwargs12个控制方问的参数。

  - requests.head() 获取HTML网页头信息 

  - requests.post() 向HTML网页提交POST请求 

  - requests.put() 向HTML网页提交PUT请求 

  - requests.patch() 向HTML网页提交局部修改请求

  - requests.delete() 向HTML网页提交删除请求

  - HTTP协议

    - 超文本传输协议，是一个基于“请求与响应”模式的、无状态的应用层协议。
    - 采用URL作为定义网络资源的标识。相当于电脑文件存储地址，一个url对应一个数据资源。

  - 爬取网页的通用代码框架

    ```
    try:
    	r = requests.get(url,timeout = 30)
    	r.raise_for_status()  #如果状态不是200，引发HTTPError异常
    	r.encoding = r.apparent_encoding
    	return r.text
    except:
    	return "产生异常"
    ```

- 爬虫的“盗亦有道”

  - 问题：骚扰问题、法律风险、隐私泄露
  - 网络爬虫限制：来源审查（判断user-agent进行限制）、robots协议
  - robots协议，告知爬虫哪些可以爬取哪些不允许。类人行为可不参考。
    - User-agent:*
    - Disallow:/

- 实例

  - 实例1：京东商品页面的爬取

    ```
    import  requests
    url = "https://item.jd.com/100002795955.html"
    try:
        r = requests.get(url)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(r.text[:1000])
    except:
        print("爬取失败")
    ```

  - 实例2：亚马逊商品页面的爬取

    ```
    #在按照京东实例方法爬取时反馈（抱歉由于程序执行时遇到意外错误，没有执行成功），是因为亚马逊会识别身份标识，当出现可以爬虫时，会拒绝执行。
    import requests
    r.request.headers
    {'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
    针对此，可以更改user-agent,改成常用浏览器身份标识。
    ____
    import requests
    url = "https://www.amazon.cn/dp/B01I6UG8WY?ref_=Oct_DLandingSV2_PC_df6829bb_0&smid=A3TEGLC21NOO5Y"
    try:
        kv = {'user-agent':'Mozilla/5.0'}
        r = requests.get(url,headers = kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(r.text[:1000])
    except:
        print("爬取失败")
    ```

  - 实例3：百度搜索关键词提交

    ```
    import requests
    keyword = "python"
    try:
        kv = {'wd':keyword}
        r = requests.get("http://www.baidu.com/s",params = kv)
        print(r.request.url)
        r.raise_for_status()
        print(len(r.text))
    except:
        print("爬取失败")
    ```

  - 实例4：网络图片的爬取和保存

    ```
    import requests
    import os
    url = "https://www.ivsky.com/download_pic.html?picurl=/img/tupian/pic/201811/23/huaping-009.jpg"
    root = "D://pics//"
    path = root + url.split("/")[-1]
    try:
        if not os.path.exists(root):
            os.mkdir(root)
        if not os.path.exists(path):
            r = requests.get(url)
            with open(path,'wb') as f:
                f.write(r.content)
                f.close()
                print("文件保存成功")
        else:
            print("文件已存在")
    except:
        print("爬取失败")
    ```

  - 实例5：ip地址归属地的查询

    ```
    import requests
    url = "http://m.ip138.com/ip.asp?ip="
    try:
        r = requests.get(url + '202.204.80.112')
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(t.text[-500:])
    except:
        print("爬取失败")
    ```

# 002-网络爬虫的提取

日期：20190826    时长：240MS    累计：390MS         

## 内容大纲

beautiful soup库入门、信息组织与提取

## 内容详解

- beautiful soup库入门

  - 获取HTML源代码，网址右键“查看网页源代码”

  - 调用beautiful soup

    ```
    from bs4 import BeautifulSoup
    soup = BeautifulSoup('<p>data</p>','html.parser')
    ```

  - 基本元素

    - Tag，标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾
    - Name，标签的名字，<p>…</p>名字是'p'，格式：<tag>.name
    - Attributes，标签的属性，字典形式组织，格式：<tag>.attrs
    - NacigableString，标签内非属性字符串，<>…</>中字符串，格式：<tag>.string
    - Comment，标签内字符串的注释部分，一种特殊的Comment类型
    - ![1566810429268](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566810429268.png)

- 基于bs4库的HTML内容遍历方法

  - HTML基本格式，树形结构
    - ![1566810661631](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566810661631.png)

  - 下行遍历

    - .contents，子节点的列表，将<tag>所有儿子节点存入列表

    - .children，子节点的迭代类型，与.contents类似，用于循环遍历儿子节点

    - .descendants，子孙节点的迭代类型，包含所有子孙节点，用于循环遍历

      ```
      from bs4 import BeautifulSoup
      soup = BeautifulSoup('<p>data</p>','html.parser')
      soup.head
      soup.head.contents
      soup.body.contents
      len(soup.body.contents)
      soup.body.contents[2]
      for child in soup.body.children:
      	print(child)  #遍历儿子节点
      ```

  - 上行遍历

    - .parent，节点的父亲标签

    - .parents，节点先辈标签的迭代类型，用于循环遍历先辈节点

      ```
      import requests
      r = requests.get("http://python123.io/ws/demo.html")
      demo = r.text
      from bs4 import BeautifulSoup
      soup = BeautifulSoup(demo,'html.parser')
      soup.title.parent
      soup.html.parent
      ```

      ```
      soup = BeautifulSoup(demo,'html.parser')
      for parent in soup.a.parents:
      	if parent is None:
      		print(parent)
      	else:
      		print(parent.name)
      ```

  - 平行遍历（发生在同一个父节点下的各节点间）

    - .next_sibling，返回按照HTML文本顺序的下一个平行节点标签

    - .previous_sibling，返回按照HTML文本顺序的上一个平行节点标签

    - .next_siblings，迭代类型，返回按照HTML文本顺序的后续所有平行节点标签

    - .previous_siblings，迭代类型，返回按照HTML文本顺序的前续所有平行节点标签

      ```
      soup = BeautifulSoup(demo,'html.parser')
      soup.a.next_sibling
      soup.a.next_sibling.next_sibling
      
      soup = BeautifulSoup(demo,'html.parser')
      for sibling in soup.a.next_siblings:
      	if parent is None:
      		print(sibling)
      
      for sibling in soup.a.previous_siblings:
      	if parent is None:
      		print(sibling)
      
      ```

      ![1566813908192](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566813908192.png)

- 基于bs4库的HTML格式输出，prettify()

  - ```
    import requests
    r = requests.get("http://python123.io/ws/demo.html")
    demo = r.text
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(demo,'html.parser')
    soup.prettify()
    print(soup.prettify())
    print(soup.a.prettify())
    ```

- 信息组织与提取

  - 标记的三种形式：XML（可扩展性好但繁琐），JSON（有类型的键值对 key:value，适合程序处理js相对简洁），YAML（无类型的键值对，文本信息比例最高，可读性好）

    - ![1566815517751](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566815517751.png)
    - ![1566815537133](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566815537133.png)
    - ![1566815567201](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566815567201.png)

  - 信息提取的一般方法

    ```
    import requests
    r = requests.get("http://python123.io/ws/demo.html")
    demo = r.text
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(demo,"html.parser")
    for link in soup.find_all('a'):
        print(link.get('href'))
    ```

  - 基于bs4库的HTML内容查找方法<>.find_all(name,attrs,recursive,string,**kwargs)返回一个列表类型，存储查找的结果。

    - Name，对标签名称的检索字符串

    - attrs，对标签属性值的检索字符串，可标注属性检索。

    - recursive，是否对子孙全部检索，默认True

    - string，<>…</>中字符串区域的检索字符串

    - 扩展方法

      ![1566817547677](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1566817547677.png)

- 实例

  - 实例1：《中国大学排名爬虫》

    - 程序的结构设计

      > 1.从网络上获取大学排名网页内容
      >
      > 2.提取网页内容中的信息到合适的数据结构
      >
      > 3.利用数据结构展示并输出结果

    - 程序

      ```
      import requests
      from bs4 import BeautifulSoup
      import bs4
      def gethtmltext(url):
          try:
              r = requests.get(url,timeout = 30)
              r.raise_for_status()
              r.encoding = r.apparent_encoding
              return r.text
          except:
              return ""
      
      def fillunivlist(ulist,html):
          soup = BeautifulSoup(html,"html.parser")
          for tr in soup.find('tbody').children:
              if isinstance(tr,bs4.element.Tag):
                  tds = tr('td')
                  ulist.append([tds[0].string,tds[1].string,tds[2].string])
      
      def printunivlist(ulist,num):
          print("{:^10}\t{:^6}\t{:^10}".format("排名","学校名称","总分"))
          for i in range(num):
              u = ulist[i]
              print("{:^10}\t{:^6}\t{:^10}".format(u[0],u[1],u[2]))
      
      def main():
          uinfo = []
          url = "https://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html"
          html = gethtmltext()
          fillunivlist(uinfo,html)
          printunivlist(uinfo,20) #前20
      main()
      ```

         

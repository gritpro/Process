# 001-网络爬虫的规则

日期：20190825    时长：150MS    累计：150MS         

## 内容大纲

Requests库入门、爬虫的“盗亦有道”、Requests库实战练习

## 内容详解

- Requests库入门

  - requests.request() 构造一个请求，支撑以下各方法的基础方法

  - requests.get() 获取HTML网页的主要方法

    - ```
      r= requests.get(url)
      #5个response对象的属性
      r.status_code  #HTTP请求的返回状态，200链接成功，404表示失败
      r.text  #url对应的网页内容
      r.encoding  #从HTTP header中猜测的响应内容编码方式，如果header中不存在charset，则认为编码为ISO-8859-1
      r.apparent_encoding  #从内容中分析出的响应内容编码方式(备选)，原则讲比r.encoding更准确
      r.content # HTTP响应内容的二进制形式
      ```

    - requests.get(url,params=None,**kwargs)，url页面链接，params是url中的额外参数，可选，kwargs12个控制方问的参数。

  - requests.head() 获取HTML网页头信息 

  - requests.post() 向HTML网页提交POST请求 

  - requests.put() 向HTML网页提交PUT请求 

  - requests.patch() 向HTML网页提交局部修改请求

  - requests.delete() 向HTML网页提交删除请求

  - HTTP协议

    - 超文本传输协议，是一个基于“请求与响应”模式的、无状态的应用层协议。
    - 采用URL作为定义网络资源的标识。相当于电脑文件存储地址，一个url对应一个数据资源。

  - 爬取网页的通用代码框架

    ```
    try:
    	r = requests.get(url,timeout = 30)
    	r.raise_for_status()  #如果状态不是200，引发HTTPError异常
    	r.encoding = r.apparent_encoding
    	return r.text
    except:
    	return "产生异常"
    ```

- 爬虫的“盗亦有道”

  - 问题：骚扰问题、法律风险、隐私泄露
  - 网络爬虫限制：来源审查（判断user-agent进行限制）、robots协议
  - robots协议，告知爬虫哪些可以爬取哪些不允许。类人行为可不参考。
    - User-agent:*
    - Disallow:/

- 实例

  - 实例1：京东商品页面的爬取

    ```
    import  requests
    url = "https://item.jd.com/100002795955.html"
    try:
        r = requests.get(url)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(r.text[:1000])
    except:
        print("爬取失败")
    ```

  - 实例2：亚马逊商品页面的爬取

    ```
    #在按照京东实例方法爬取时反馈（抱歉由于程序执行时遇到意外错误，没有执行成功），是因为亚马逊会识别身份标识，当出现可以爬虫时，会拒绝执行。
    import requests
    r.request.headers
    {'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
    针对此，可以更改user-agent,改成常用浏览器身份标识。
    ____
    import requests
    url = "https://www.amazon.cn/dp/B01I6UG8WY?ref_=Oct_DLandingSV2_PC_df6829bb_0&smid=A3TEGLC21NOO5Y"
    try:
        kv = {'user-agent':'Mozilla/5.0'}
        r = requests.get(url,headers = kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(r.text[:1000])
    except:
        print("爬取失败")
    ```

  - 实例3：百度搜索关键词提交

    ```
    import requests
    keyword = "python"
    try:
        kv = {'wd':keyword}
        r = requests.get("http://www.baidu.com/s",params = kv)
        print(r.request.url)
        r.raise_for_status()
        print(len(r.text))
    except:
        print("爬取失败")
    ```

  - 实例4：网络图片的爬取和保存

    ```
    import requests
    import os
    url = "https://www.ivsky.com/download_pic.html?picurl=/img/tupian/pic/201811/23/huaping-009.jpg"
    root = "D://pics//"
    path = root + url.split("/")[-1]
    try:
        if not os.path.exists(root):
            os.mkdir(root)
        if not os.path.exists(path):
            r = requests.get(url)
            with open(path,'wb') as f:
                f.write(r.content)
                f.close()
                print("文件保存成功")
        else:
            print("文件已存在")
    except:
        print("爬取失败")
    ```

  - 实例5：ip地址归属地的查询

    ```
    import requests
    url = "http://m.ip138.com/ip.asp?ip="
    try:
        r = requests.get(url + '202.204.80.112')
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(t.text[-500:])
    except:
        print("爬取失败")
    ```

    
